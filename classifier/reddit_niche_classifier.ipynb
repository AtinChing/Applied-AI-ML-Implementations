{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e67769b",
   "metadata": {},
   "source": [
    "Proper about me can be made later\n",
    "Classifier that classifies what niche category a certain reddit post falls into.\n",
    "\n",
    "First, we need to collect data.\n",
    "There aren't many very good datasets, so we need to create our own.\n",
    "This will be done through data scraping via PRAW and weak supervision via a chosen LLM (I am using Gemini for this).\n",
    "\n",
    "First, scraping data via PRAW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9773d9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required dependencies\n",
    "\n",
    "%pip install -r requirements.txt --user # --user flag is needed because one of the dependencies (google-genai) needs to access a script that is hidden in non-administrator environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f954a574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make your necessary imports\n",
    "import praw\n",
    "import pandas as pd\n",
    "from google import genai\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926c71a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize reddit client session\n",
    "\n",
    "CLIENT_ID = \"0xeiOSktNDiHBw\"\n",
    "CLIENT_SECRET = \"c-bNB_P5wRjHZmaD1eaJnx0D3mlr8Q\"\n",
    "USER_AGENT = \"sestee 1.0\"\n",
    "cli = praw.Reddit(\n",
    "        client_id=CLIENT_ID,\n",
    "        client_secret=CLIENT_SECRET,\n",
    "        user_agent=USER_AGENT\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98feecc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare a way for you to scrape posts from a subreddit of your choice.\n",
    "def scrape_popular_posts(subreddits, limit=100, sort_by=\"top\"):\n",
    "    posts = []\n",
    "\n",
    "    for sub_name in subreddits:\n",
    "        subreddit = cli.subreddit(sub_name)\n",
    "\n",
    "        if sort_by == \"top\":\n",
    "            submissions = subreddit.top(limit=limit)\n",
    "        elif sort_by == \"hot\":\n",
    "            submissions = subreddit.hot(limit=limit)\n",
    "        elif sort_by == \"new\":\n",
    "            submissions = subreddit.new(limit=limit)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid sort_by value. Use 'top', 'hot', or 'new'.\")\n",
    "\n",
    "        for post in submissions:\n",
    "            post_data = {\n",
    "                \"title\": post.title,\n",
    "                \"selftext\": post.selftext,\n",
    "                \"subreddit\": post.subreddit.display_name,\n",
    "                \"flair\": post.link_flair_text,\n",
    "                \"score\": post.score,\n",
    "                \"num_comments\": post.num_comments,\n",
    "                \"upvote_ratio\": post.upvote_ratio,\n",
    "                \"created_utc\": post.created_utc,\n",
    "                \"id\": post.id,\n",
    "                \"url\": post.url\n",
    "            }\n",
    "            posts.append(post_data)\n",
    "\n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abb67e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out what subreddits you want to scrape from\n",
    "subreddits = [\"AskReddit\", \"relationships\", \"AmItheAsshole\", \"TrueOffMyChest\", \"TIFU\"]\n",
    "# Scrape the data from the subreddits\n",
    "data = scrape_popular_posts(subreddits, limit=50, sort_by=\"top\")\n",
    "# Save the data in a pandas dataframe\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Can save the dataframe to a CSV file too!\n",
    "#df.to_csv(\"reddit_posts.csv\", index=False)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f406ef9f",
   "metadata": {},
   "source": [
    "Now we have a good chunk of all the data that we need. We need to clean it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d50082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the data\n",
    "# Check and do later\n",
    "#df = df.dropna(subset=[\"selftext\", \"title\"])  # Drop rows with NaN in 'selftext' or 'title'\n",
    "#df = df[df[\"selftext\"].str.strip() != \"\"]  # Drop empty 'selftext' rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7e90be",
   "metadata": {},
   "source": [
    "Now that we have our data, we will create a pipeline that allows us to label all the data entries and add a \"niche\" column via weak supervision. All entries will then be classified.\n",
    "These are the post classification categories we are planning to classify our posts into.\n",
    "\n",
    "| Label         | Description                                |\n",
    "|---------------|--------------------------------------------|\n",
    "| `advice`      | Help-seeking posts, questions, dilemmas    |\n",
    "| `story`       | Personal anecdotes with a beginning, middle, end |\n",
    "| `drama`       | High-stakes conflict, betrayal, gossip      |\n",
    "| `rant`        | Emotional venting or unfiltered frustration |\n",
    "| `humor`       | Meme-like, comedic, shitpost-style content  |\n",
    "| `informative` | Tips, how-tos, PSAs, educational content    |\n",
    "| `confession`  | Vulnerable personal reveals or identity-based confessions |\n",
    "| `unknown`     | Doesnâ€™t fit confidently into other categories|\n",
    "\n",
    "Note: We can use the `unknown` category to find the biggest weaknesses of our LLM, and we can then possibly fine-tune our LLM later very efficiently by especially targetting its weaknesses that we've detected here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd1549e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI learns patterns from data to make predictions or decisions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the Google GenAI API client\n",
    "client = genai.Client(api_key=\"AIzaSyDSyIBzIJ9yVnXYd6sJaE7oZ0Vqnc4kEPM\")\n",
    "model = \"gemini-2.0-flash\" # There are a LOT of models to choose from. But in my experience, I feel comfortable with AND use 2.0-flash the most. Will look into 2.5 series once they go through stable release.\n",
    "contents = \"\"\"I want to train a transofmer-based classifer that takes in the text of a reddit post and then classifes them into labels [personal advice, story, drama]. I only have a partial dataset for this. Can you help fill the rest for me?\n",
    "It should JUST classify the post into one niche category. Make me 75 rows in this dataset with equal category distribution. Do not stop until you are done\"\"\"\n",
    "response = client.models.generate_content(\n",
    "    model=model, contents=\"Explain how AI works in a few words\"\n",
    ")\n",
    "print(response.text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
