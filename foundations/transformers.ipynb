{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfomers\n",
    "- Transformers are perhaps one of the most special, impactful and important creations in AI/Computer Science of **all time**.\n",
    "- Transformer models are used to solve all kinds of tasks across different modalities, including natural language processing (NLP), computer vision, audio processing, and more.\n",
    "\n",
    "### Hugging Face Transformers Library\n",
    "\n",
    "The **Hugging Face Transformers** library is the most important and widely-used library for working with transformer models in the AI/ML ecosystem.\n",
    "\n",
    "### What is it?\n",
    "- A comprehensive Python library that provides easy access to thousands of pre-trained transformer models\n",
    "- Supports all major transformer architectures: BERT, GPT, T5, RoBERTa, DistilBERT, and many more\n",
    "- Provides a unified, consistent API regardless of the underlying model architecture\n",
    "- Includes tools for fine-tuning, training from scratch, and deploying models to production\n",
    "\n",
    "### Why it's significant\n",
    "The library democratized AI by making state-of-the-art transformer models accessible to developers, researchers, and companies worldwide. It created a standardized interface that works across different model families and tasks, building the largest model hub with 100,000+ pre-trained models shared by the community. This massive ecosystem is now used by thousands of companies in real-world applications.\n",
    "\n",
    "### The Pipeline Function - The Game Changer\n",
    "\n",
    "- The `pipeline()` function is the most simple object in the transformers library, yet perhaps the most revolutionary feature - it's like having a \"one-click\" solution for AI tasks. It handles tokenization, model loading, and post-processing automatically, supporting 20+ different tasks (classification, generation, translation, summarization, etc.) with zero configuration needed.\n",
    "- You can think of it as something that connects a model with its necessary preprocessing and postprocessing steps. This lets us directly input any text and get an intelligible answer.\n",
    "- Below I will show you an example of sentiment analysis using the pipeline function, on our own inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from -r transformers_requirements.txt (line 1)) (0.8.1)\n",
      "Requirement already satisfied: torch in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from -r transformers_requirements.txt (line 3)) (2.7.1)\n",
      "Collecting matplotlib (from -r transformers_requirements.txt (line 4))\n",
      "  Using cached matplotlib-3.10.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting seaborn (from -r transformers_requirements.txt (line 5))\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: transformers[sentencepiece] in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from -r transformers_requirements.txt (line 2)) (4.53.0)\n",
      "Requirement already satisfied: filelock in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: protobuf in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (6.31.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (1.1.5)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from torch->-r transformers_requirements.txt (line 3)) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from torch->-r transformers_requirements.txt (line 3)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from torch->-r transformers_requirements.txt (line 3)) (3.1.6)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->-r transformers_requirements.txt (line 4))\n",
      "  Using cached contourpy-1.3.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->-r transformers_requirements.txt (line 4))\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->-r transformers_requirements.txt (line 4))\n",
      "  Using cached fonttools-4.58.4-cp310-cp310-macosx_10_9_universal2.whl.metadata (106 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->-r transformers_requirements.txt (line 4))\n",
      "  Using cached kiwisolver-1.4.8-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.2 kB)\n",
      "Collecting pillow>=8 (from matplotlib->-r transformers_requirements.txt (line 4))\n",
      "  Using cached pillow-11.2.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (8.9 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib->-r transformers_requirements.txt (line 4))\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from matplotlib->-r transformers_requirements.txt (line 4)) (2.9.0.post0)\n",
      "Collecting pandas>=1.2 (from seaborn->-r transformers_requirements.txt (line 5))\n",
      "  Using cached pandas-2.3.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Collecting pytz>=2020.1 (from pandas>=1.2->seaborn->-r transformers_requirements.txt (line 5))\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=1.2->seaborn->-r transformers_requirements.txt (line 5))\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->-r transformers_requirements.txt (line 4)) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from sympy>=1.13.3->torch->-r transformers_requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from jinja2->torch->-r transformers_requirements.txt (line 3)) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from requests->transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from requests->transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from requests->transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from requests->transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (2025.6.15)\n",
      "Downloading matplotlib-3.10.3-cp310-cp310-macosx_11_0_arm64.whl (8.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading contourpy-1.3.2-cp310-cp310-macosx_11_0_arm64.whl (253 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.58.4-cp310-cp310-macosx_10_9_universal2.whl (2.7 MB)\n",
      "Using cached kiwisolver-1.4.8-cp310-cp310-macosx_11_0_arm64.whl (65 kB)\n",
      "Using cached pandas-2.3.0-cp310-cp310-macosx_11_0_arm64.whl (10.8 MB)\n",
      "Using cached pillow-11.2.1-cp310-cp310-macosx_11_0_arm64.whl (3.0 MB)\n",
      "Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, pandas, matplotlib, seaborn\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/11\u001b[0m [seaborn]9/11\u001b[0m [matplotlib]\n",
      "\u001b[1A\u001b[2KSuccessfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.58.4 kiwisolver-1.4.8 matplotlib-3.10.3 pandas-2.3.0 pillow-11.2.1 pyparsing-3.2.3 pytz-2025.2 seaborn-0.13.2 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r transformers_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9931273460388184}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\") # there are SO many differents tasks you could name here\n",
    "# by default, a particular pretrained model that has been fine-tuned for sentiment analysis in English gets chosen\n",
    "# here, sentinment-analysis defaults to distilbert-base-uncased-finetuned-sst-2-english,\n",
    "# which is part of the BERT model lines (by Google)\n",
    "classifier(\"I'm so hungry man!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9961236119270325},\n",
       " {'label': 'POSITIVE', 'score': 0.9998375177383423}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can even do multiple outputs\n",
    "classifier(\n",
    "    [\"I'm so hungry man.\", \"I love food so much.\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth noting that the model gets downloaded and cached when you create the classifier object. Now, everytime you rerun the command, the cached model gets used instead, no need for repeated downloads.\n",
    "Here's what happens everytime you pass some text into a pipeline:\n",
    "- The text is preprocessed into a format the model can understand.\n",
    "- The preprocessed inputs are passed to the model.\n",
    "- The predictions of the model are post-processed, so you can make sense of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also worth noting the different tasks you can do with the pipeline object. Below are some examples:\n",
    "\n",
    "Text pipelines\n",
    "- text-generation: Generate text from a prompt\n",
    "- text-classification: Classify text into predefined categories\n",
    "- summarization: Create a shorter version of a text while preserving key information\n",
    "- translation: Translate text from one language to another\n",
    "- zero-shot-classification: Classify text without prior training on specific labels\n",
    "- feature-extraction: Extract vector representations of text\n",
    "\n",
    "Image pipelines\n",
    "- image-to-text: Generate text descriptions of images\n",
    "- image-classification: Identify objects in an image\n",
    "- object-detection: Locate and identify objects in images\n",
    "\n",
    "Audio pipelines\n",
    "- automatic-speech-recognition: Convert speech to text\n",
    "- audio-classification: Classify audio into categories\n",
    "- text-to-speech: Convert text to spoken audio\n",
    "\n",
    "Multimodal pipelines\n",
    "- image-text-to-text: Respond to an image based on a text prompt\n",
    "\n",
    "Now, lets try classifying texts that haven't been labelled. We're going to use something called zero-shot classification, as it allows us to specify which labels to use for classification, so we don't have to rely on pretrained models' labels. \n",
    "- (The pipeline is called zero-shot because you don’t need to fine-tune the model on your data to use it. It can directly return probability scores for any list of labels you want.)\n",
    "\n",
    "In the above example, we classified using 2 labels: positive and negative. But now I'll show you how to classify text using ANY set of labels of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': \"History isn't really a fun class\",\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.7267491817474365, 0.19325977563858032, 0.07999099045991898]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier(\n",
    "    \"History isn't really a fun class\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also choose a specific model for a specific task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"So today, I just feel like I need to make sure that I am getting all of my 2016 goals accomplished. \\xa0I have my goals for 2016, but I didn't get everything accomplished. \\xa0So, I want to make sure that I can see what I have accomplished.\\n\\nI need to get more into the habit of writing in my journal, I think that will help me get through a lot of things. \\xa0I know that when I feel like I am not feeling good, I can just go into my journal and take a look at what I should be doing. \\xa0It will help me see where I need to improve.\\n\\nI am going to post the 2016 goals that I have listed in this post. \\xa0I have listed the goals for each day, and will have a few goals for each week and month.\\n\\nI want to be able to see these goals, so I can see where I am and where I need to improve. \\xa0I know that if I don't see these, I will feel discouraged and will not be able to get through a lot of things.\\n\\nI want to be able to see how far I have come, so even if I didn't accomplish\"},\n",
       " {'generated_text': \"So today, I just feel like I'm being told what to do by my own government. I'm tired of the government telling me what I can do in my own home.\\n\\nThis is a scary thing.\\n\\nAnd I'm just so tired of all the BS that's going on right now.\\n\\nI thought we were living in a world where we could be free of government now. After all, we've been over this before. We've had our rights and freedoms. We've had our rights and freedoms taken away. And now, we're being told what to do by the government.\\n\\nI know this isn't a new concept. I've been telling my kids this for years. It's been on the news. It's on the radio. It's being taught in our schools. And I'm tired of it.\\n\\nI want to be free of government. I want to be free of this government tyranny. I want to be free of the government that tells me what to do.\\n\\nI want to be free of this government that is telling me what to do.\\n\\nI want to be free of this government that tells me what to do.\\n\\nI want to be free of this government that tells me what to do.\\n\\nI\"}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"HuggingFaceTB/SmolLM2-360M\") # all these models names\n",
    "# are names directly from the hugging face hub's website. You can plug in ANY model from their hub.\n",
    "# It includes all possible basic models too, that you could ever think of.\n",
    "\n",
    "# This is text generation, where you just provide some prompt, and the model auto-completes it by generating the remaining text.\n",
    "# Similar to text predictions you see on your phone.\n",
    "# Text generation is random, and it is unlikely you will get the exact same responses every time.\n",
    "\n",
    "generator(\n",
    "    \"So today, I just feel like\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are SO many other tasks and examples I could show you, that I will not be exhaustively going over them here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining data from multiple sources\n",
    "One powerful application of Transformer models is their ability to combine and process data from multiple sources. This is especially useful when you need to:\n",
    "\n",
    "- Search across multiple databases or repositories\n",
    "- Consolidate information from different formats (text, images, audio)\n",
    "- Create a unified view of related information\n",
    "\n",
    "For example, you could build a system that:\n",
    "- Searches for information across databases in multiple modalities like text and image.\n",
    "- Combines results from different sources into a single coherent response. For example, from an audio file and text description.\n",
    "- Presents the most relevant information from a database of documents and metadata.\n",
    "\n",
    "This is an import aspect of transformers, and is just something worth noting.\n",
    "Next, we're going to get into the really good stuff, the nitty-gritty, how transformers ACTUALLY work, whats goes on in the inside, and transformer model architecture, in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Architecture\n",
    "- Transformer architecture is mainly composed of 2 blocks/layers/models:\n",
    "  - Encoder layer (left): The encoder receives an input and builds a representation of it (its features). This means that the model is optimized to acquire understanding from the input.\n",
    "  - Decoder (right): The decoder uses the encoder’s representation (features) along with other inputs to generate a target sequence. This means that the model is optimized for generating outputs. \n",
    "- Here's an image for visualisation\n",
    "\n",
    "![Transformer Architecture Visualisation](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks-dark.svg)\n",
    "- Each of these parts can be used independently too, depending on the task:\n",
    "  - Encoder-only models: Good for tasks that require understanding of the input, such as sentence classification and named entity recognition.\n",
    "  - Decoder-only models: Good for generative tasks such as text generation.\n",
    "  - Encoder-decoder models or sequence-to-sequence models: Good for generative tasks that require an input, such as translation or summarization (what you see with pure ChatGPT).\n",
    "- Another key aspect of Transformer models are **Attention layers**.\n",
    "  - By the way, the transformer architecture was introduced in a Google paper titled \"Attention is All You Need\". Says a lot.\n",
    "-  This layer tells the model to pay attention **specifically** to certain parts of the data (e.g. with LLMs, that would be words in the sentence) that you passed it (and more or less ignore the others), when dealing with the representation of each piece of data (word). \n",
    "    - Here's an example: imagine you have to translate \"You like this house\" from English to French. The translation model will need to look at \"you\" to get the proper translation for \"like\", because in French, the word \"like\" is written differently depending on the subject. But, the rest of the sentence is not useful for the translation of \"like\".\n",
    "    - In the same way, when translating “this” the model will also need to pay attention to the word \"house\", because “this” translates differently, depending on if the noun is masculine or feminine. Again, the other words in the sentence will not matter for the translation of “house”. \n",
    "    - With more complex sentences (and more complex grammar rules), the model would need to pay special attention to words that might appear farther away in the sentence to properly translate each word.\n",
    "    - This same concept applies to ANY task at ANY scale associated with natural language: a word by itself has a meaning, but that meaning is deeply affected by the context, which can be any other word (or words) before or after the word being studied.\n",
    "    - Here's a code visualization and application of our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Forward pass to get encoder attentions\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 20\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Get translation\u001b[39;00m\n\u001b[1;32m     23\u001b[0m translated \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n",
      "File \u001b[0;32m~/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py:1548\u001b[0m, in \u001b[0;36mMarianMTModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[1;32m   1545\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m   1546\u001b[0m         )\n\u001b[0;32m-> 1548\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1561\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1562\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1566\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_logits_bias\n\u001b[1;32m   1568\u001b[0m masked_lm_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py:1301\u001b[0m, in \u001b[0;36mMarianModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1294\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[1;32m   1295\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   1296\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1297\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1298\u001b[0m     )\n\u001b[1;32m   1300\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1301\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1318\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[0;32m~/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py:994\u001b[0m, in \u001b[0;36mMarianDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;66;03m# retrieve input_ids and inputs_embeds\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m^\u001b[39m (inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 994\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    996\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m input_ids\n",
      "\u001b[0;31mValueError\u001b[0m: You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Prepare input\n",
    "src_text = \"You like this house\"\n",
    "inputs = tokenizer(src_text, return_tensors=\"pt\")\n",
    "\n",
    "# Set attention implementation to eager (must be done before forward)\n",
    "model.config.attn_implementation = \"eager\"\n",
    "\n",
    "# Forward pass to get encoder attentions\n",
    "with torch.no_grad():\n",
    "    output = model(**inputs, output_attentions=True)\n",
    "\n",
    "# Get translation\n",
    "translated = model.generate(**inputs)\n",
    "fr_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "print(\"French translation:\", fr_text)\n",
    "\n",
    "# Get attention from last encoder layer\n",
    "attentions = output.encoder_attentions[-1][0]  # shape: [num_heads, seq_len, seq_len]\n",
    "\n",
    "# Average over heads for visualization\n",
    "avg_attn = attentions.mean(dim=0)\n",
    "\n",
    "# Clean token labels\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "tokens = [tok for tok in tokens if tok not in (\"</s>\", \"<pad>\")]\n",
    "avg_attn = avg_attn[:len(tokens), :len(tokens)]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(avg_attn.numpy(), xticklabels=tokens, yticklabels=tokens, cmap=\"viridis\", annot=True, fmt=\".2f\")\n",
    "plt.title(\"Average Encoder Self-Attention (Last Layer)\")\n",
    "plt.xlabel(\"Key\")\n",
    "plt.ylabel(\"Query\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the above image, we see a self-attention matrix from BERT (Layer 1, Head 1). Here’s how to read it:\n",
    "\t-\tY-axis (Query): The token whose attention is being calculated (e.g. “like”, “house”).\n",
    "\t-\tX-axis (Key): The tokens that receive attention from the query.\n",
    "\t-\tColor intensity: Strength of attention weight (brighter = more attention).\n",
    "\n",
    "In short, a row shows which tokens a particular word is focusing on.\n",
    "- To further accelerate our understanding, we will briefly look at the original transformer architecture:\n",
    "  - Original transformer architecture was designed for translation.\n",
    "  - During training, the encoder receives inputs (sentences) in a certain language, while the decoder receives the same sentences in the desired target language. \n",
    "  - In the encoder, the attention layers can use all the words in a sentence (since, as we just saw, the translation of a given word can be dependent on what is after as well as before it in the sentence). \n",
    "  - The decoder, however, works sequentially and can only pay attention to the words in the sentence that it has already translated (so, only the words before the word currently being generated). \n",
    "    - For example, when we have predicted the first three words of the translated target, we give them to the decoder which then uses all the inputs of the encoder to try to predict the fourth word.\n",
    "  - To speed things up during training (when the model has access to target sentences), the decoder is fed the whole target, but it is not allowed to use future words (if it had access to the word at position 2 when trying to predict the word at position 2, the problem would not be very hard!).\n",
    "  - For instance, when trying to predict the fourth word, the attention layer will only have access to the words in positions 1 to 3.\n",
    "  - Here's a visualisation of the original Transformer architecture, with the encoder on the left and the decoder on the right:\n",
    "![Original Transformer Architecture Visualisation](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers-dark.svg)\n",
    "  - Note that the first attention layer in a decoder block pays attention to all (past) inputs to the decoder, but the second attention layer uses the output of the encoder. It can thus access the whole input sentence to best predict the current word.\n",
    "    - This is very useful as different languages can have grammatical rules that put the words in different orders, or some context provided later in the sentence may be helpful to determine the best translation of a given word.\n",
    "  - The attention mask can also be used in the encoder/decoder to prevent the model from paying attention to some special words — for instance, the special padding word used to make all the inputs the same length when batching together sentences.\n",
    "  - Hopefully, this gave you a solid mental model of how the original Transformer architecture works — particularly the encoder-decoder setup and how attention mechanisms shape language understanding and generation. We covered this not just for historical context, but because these components (like self-attention, masking, and encoder-decoder flows) form the foundation of nearly all modern Transformer-based models. \n",
    "    - Understanding them now will make it much easier to grasp variants like GPT, BERT, or T5, and to debug, fine-tune, or even build your own architectures later on.\n",
    "  #### Architectures and checkpoints\n",
    "  - This is terminology that is important to know for now and later. You’ll see mentions of architectures and checkpoints as well as models. These terms all have slightly different meanings:\n",
    "    - **Architecture**: This is the skeleton of the model — the definition of each layer and each operation that happens within the model.\n",
    "    - **Checkpoints**: These are the weights that will be loaded in a given architecture.\n",
    "    - **Model**: This is an umbrella term that isn’t as precise as “architecture” or “checkpoint\": it can mean both. This course will specify architecture or checkpoint when it matters to reduce ambiguity.\n",
    "    - Example: BERT is an architecture while bert-base-cased, a set of weights trained by the Google team for the first release of BERT, is a checkpoint. However, one can say “the BERT model” and “the bert-base-cased model.”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How transformers solve their tasks\n",
    "- Now, we will delve into specific transformer architectural variants, and how they solve their respective, specific tasks. Before we do this however, its important to understand that most tasks follow a similar pattern: input data is processed through a model, and the output is interpreted for a specific task. The differences lie in how the data is prepared, what model architecture variant is used, and how the output is processed.\n",
    "- To explain how tasks are solved, we’ll walk through what goes on inside the model to output useful predictions. We’ll cover the following models and their corresponding tasks:\n",
    "\n",
    "  - Wav2Vec2 for audio classification and automatic speech recognition (ASR)\n",
    "  - Vision Transformer (ViT) and ConvNeXT for image classification\n",
    "  - DETR for object detection\n",
    "  - Mask2Former for image segmentation\n",
    "  - GLPN for depth estimation\n",
    "  - BERT for NLP tasks like text classification, token classification and question answering that use an encoder\n",
    "  - GPT2 for NLP tasks like text generation that use a decoder\n",
    "  - BART for NLP tasks like summarization and translation that use an encoder-decoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Language models and the most famous examples:\n",
    "- All the popular Transformer models (GPT, BERT, T5, etc.) have been trained as language models. \n",
    "- Language models are just models that have been trained on large amounts of raw text in a self-supervised fashion.\n",
    "- (Self-supervised learning is a type of training in which the objective is automatically computed from the inputs of the model. That means that humans are not needed to label the data!)\n",
    "- This type of model develops a statistical understanding of the language it has been trained on, but it’s less useful for specific practical tasks. Because of this, the general pretrained model then goes through a process called transfer learning or fine-tuning. During this process, the model is fine-tuned in a supervised way — that is, using human-annotated labels — on a given task.\n",
    "- An example of a task is predicting the next word in a sentence having read the n previous words. This is called causal language modeling because the output depends on the past and present inputs, but not the future ones.\n",
    "- Example below:\n",
    "\n",
    "![Causal language modeling visualization](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling-dark.svg)\n",
    "\n",
    "- Additionally, the general strategy to achieve better performance is by increasing the models’ sizes as well as the amount of data they are pretrained on.\n",
    "- Unfortunately, training a model, especially a large one, requires a large amount of data. This becomes very costly in terms of time and compute resources.\n",
    "- This means sharing models/resources is very efficient and optimal, because it saves time, potentially money, and just overall resources for everyone.\n",
    "\n",
    "## Transfer Learning\n",
    "- Initializing a model with another model's weights.\n",
    "- Essentially, you leverage the knowledge acquired by a model trained on LOTS of data on another task.\n",
    "### Pre-training\n",
    "- **Pretraining**: training a model from scratch. The weights are initially randomly initialized, and the training starts with 0 knowledge.\n",
    "  - This pretraining is usually done on very large amounts of data. Therefore, it requires a very large corpus of data, and training can take up to several weeks.\n",
    "- **Fine-tuning**: training done AFTER a model has been pretrained. To perform fine-tuning, you first need a pretrained language model, then need to perform additional training with a dataset highly specific to your task.\n",
    "  - Now, this might seem confusing. Just train your model for your final use case from the start right? Here's some reasons why you do things this way:\n",
    "  - The pretrained model was already trained on a dataset that has some similarities with the fine-tuning dataset. The fine-tuning process is thus able to take advantage of knowledge acquired by the initial model during pretraining (for instance, with NLP problems, the pretrained model will have some kind of statistical understanding of the language you are using for your task).\n",
    "  - Since the pretrained model was already trained on lots of data, the fine-tuning requires way less data to get decent results.\n",
    "  - For the same reason, the amount of time and resources needed to get good results are much lower.\n",
    "  - Fine-tuning example:\n",
    "    - You can take a pretrained model (that was thoroughly trained on the English language), and fine-tune it on the arXiv corpus, which then results in a science/research-based model.\n",
    "      - Corpus: a large and structured collection of text or speech data used for linguistic analysis and training ML models. In NLP/LLM context, it acts as the training data for our models that need to generate human language. \n",
    "      - arXiv corpus: arXiv is an archive of scholarly articles, primarily in scientific fields. So the arXiv corpus is a collection of scholarly articles available on the arXiv repository. \n",
    "    - The fine-tuning will only require a limited amount of data. The knowledge the pretrained model has acquired is “transferred,” (You’re transferring the capabilities of the pretrained model to a new domain/task using the arXiv data), hence the term transfer learning.\n",
    "- Important to note, that transfer learning **ENCOMPASSES** fine-tuning.\n",
    "- In most situations, fine-tuning revolves around retraining the last couple layers of a model, but can be applied to the whole model.  \n",
    "- Fine-tuning a model has lower time, data, financial, and environmental costs. It is also quicker and easier to iterate over different fine-tuning schemes (full fine-tuning, last-layer tuning, freezing first n-layers and training/readjusting the rest, peft, etc), as the training is less constraining (cheaper, faster, easier to experiment with) than a full pretraining.\n",
    "- This process will also achieve better results than training from scratch (unless you have lots of data), which is why you should always try to leverage a pretrained model — one as close as possible to the task you have at hand — and fine-tune it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
