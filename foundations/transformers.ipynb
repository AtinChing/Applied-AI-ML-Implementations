{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfomers\n",
    "- Transformers are perhaps one of the most special, impactful and important creations in AI/Computer Science of **all time**.\n",
    "- Transformer models are used to solve all kinds of tasks across different modalities, including natural language processing (NLP), computer vision, audio processing, and more.\n",
    "\n",
    "### Hugging Face Transformers Library\n",
    "\n",
    "The **Hugging Face Transformers** library is the most important and widely-used library for working with transformer models in the AI/ML ecosystem.\n",
    "\n",
    "### What is it?\n",
    "- A comprehensive Python library that provides easy access to thousands of pre-trained transformer models\n",
    "- Supports all major transformer architectures: BERT, GPT, T5, RoBERTa, DistilBERT, and many more\n",
    "- Provides a unified, consistent API regardless of the underlying model architecture\n",
    "- Includes tools for fine-tuning, training from scratch, and deploying models to production\n",
    "\n",
    "### Why it's significant\n",
    "The library democratized AI by making state-of-the-art transformer models accessible to developers, researchers, and companies worldwide. It created a standardized interface that works across different model families and tasks, building the largest model hub with 100,000+ pre-trained models shared by the community. This massive ecosystem is now used by thousands of companies in real-world applications.\n",
    "\n",
    "### The Pipeline Function - The Game Changer\n",
    "\n",
    "- The `pipeline()` function is the most simple object in the transformers library, yet perhaps the most revolutionary feature - it's like having a \"one-click\" solution for AI tasks. It handles tokenization, model loading, and post-processing automatically, supporting 20+ different tasks (classification, generation, translation, summarization, etc.) with zero configuration needed.\n",
    "- You can think of it as something that connects a model with its necessary preprocessing and postprocessing steps. This lets us directly input any text and get an intelligible answer.\n",
    "- Below I will show you an example of sentiment analysis using the pipeline function, on our own inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from -r transformers_requirements.txt (line 1)) (0.8.1)\n",
      "Requirement already satisfied: torch in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from -r transformers_requirements.txt (line 3)) (2.7.1)\n",
      "Collecting matplotlib (from -r transformers_requirements.txt (line 4))\n",
      "  Using cached matplotlib-3.10.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting seaborn (from -r transformers_requirements.txt (line 5))\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: transformers[sentencepiece] in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from -r transformers_requirements.txt (line 2)) (4.53.0)\n",
      "Requirement already satisfied: filelock in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: protobuf in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (6.31.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (1.1.5)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from torch->-r transformers_requirements.txt (line 3)) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from torch->-r transformers_requirements.txt (line 3)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from torch->-r transformers_requirements.txt (line 3)) (3.1.6)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->-r transformers_requirements.txt (line 4))\n",
      "  Using cached contourpy-1.3.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->-r transformers_requirements.txt (line 4))\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->-r transformers_requirements.txt (line 4))\n",
      "  Using cached fonttools-4.58.4-cp310-cp310-macosx_10_9_universal2.whl.metadata (106 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->-r transformers_requirements.txt (line 4))\n",
      "  Using cached kiwisolver-1.4.8-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.2 kB)\n",
      "Collecting pillow>=8 (from matplotlib->-r transformers_requirements.txt (line 4))\n",
      "  Using cached pillow-11.2.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (8.9 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib->-r transformers_requirements.txt (line 4))\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from matplotlib->-r transformers_requirements.txt (line 4)) (2.9.0.post0)\n",
      "Collecting pandas>=1.2 (from seaborn->-r transformers_requirements.txt (line 5))\n",
      "  Using cached pandas-2.3.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Collecting pytz>=2020.1 (from pandas>=1.2->seaborn->-r transformers_requirements.txt (line 5))\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=1.2->seaborn->-r transformers_requirements.txt (line 5))\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->-r transformers_requirements.txt (line 4)) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from sympy>=1.13.3->torch->-r transformers_requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from jinja2->torch->-r transformers_requirements.txt (line 3)) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from requests->transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from requests->transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from requests->transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from requests->transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (2025.6.15)\n",
      "Downloading matplotlib-3.10.3-cp310-cp310-macosx_11_0_arm64.whl (8.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading contourpy-1.3.2-cp310-cp310-macosx_11_0_arm64.whl (253 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.58.4-cp310-cp310-macosx_10_9_universal2.whl (2.7 MB)\n",
      "Using cached kiwisolver-1.4.8-cp310-cp310-macosx_11_0_arm64.whl (65 kB)\n",
      "Using cached pandas-2.3.0-cp310-cp310-macosx_11_0_arm64.whl (10.8 MB)\n",
      "Using cached pillow-11.2.1-cp310-cp310-macosx_11_0_arm64.whl (3.0 MB)\n",
      "Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, pandas, matplotlib, seaborn\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11/11\u001b[0m [seaborn]9/11\u001b[0m [matplotlib]\n",
      "\u001b[1A\u001b[2KSuccessfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.58.4 kiwisolver-1.4.8 matplotlib-3.10.3 pandas-2.3.0 pillow-11.2.1 pyparsing-3.2.3 pytz-2025.2 seaborn-0.13.2 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r transformers_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9931273460388184}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\") # there are SO many differents tasks you could name here\n",
    "# by default, a particular pretrained model that has been fine-tuned for sentiment analysis in English gets chosen\n",
    "# here, sentinment-analysis defaults to distilbert-base-uncased-finetuned-sst-2-english,\n",
    "# which is part of the BERT model lines (by Google)\n",
    "classifier(\"I'm so hungry man!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9961236119270325},\n",
       " {'label': 'POSITIVE', 'score': 0.9998375177383423}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can even do multiple outputs\n",
    "classifier(\n",
    "    [\"I'm so hungry man.\", \"I love food so much.\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth noting that the model gets downloaded and cached when you create the classifier object. Now, everytime you rerun the command, the cached model gets used instead, no need for repeated downloads.\n",
    "Here's what happens everytime you pass some text into a pipeline:\n",
    "- The text is preprocessed into a format the model can understand.\n",
    "- The preprocessed inputs are passed to the model.\n",
    "- The predictions of the model are post-processed, so you can make sense of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also worth noting the different tasks you can do with the pipeline object. Below are some examples:\n",
    "\n",
    "Text pipelines\n",
    "- text-generation: Generate text from a prompt\n",
    "- text-classification: Classify text into predefined categories\n",
    "- summarization: Create a shorter version of a text while preserving key information\n",
    "- translation: Translate text from one language to another\n",
    "- zero-shot-classification: Classify text without prior training on specific labels\n",
    "- feature-extraction: Extract vector representations of text\n",
    "\n",
    "Image pipelines\n",
    "- image-to-text: Generate text descriptions of images\n",
    "- image-classification: Identify objects in an image\n",
    "- object-detection: Locate and identify objects in images\n",
    "\n",
    "Audio pipelines\n",
    "- automatic-speech-recognition: Convert speech to text\n",
    "- audio-classification: Classify audio into categories\n",
    "- text-to-speech: Convert text to spoken audio\n",
    "\n",
    "Multimodal pipelines\n",
    "- image-text-to-text: Respond to an image based on a text prompt\n",
    "\n",
    "Now, lets try classifying texts that haven't been labelled. We're going to use something called zero-shot classification, as it allows us to specify which labels to use for classification, so we don't have to rely on pretrained models' labels. \n",
    "- (The pipeline is called zero-shot because you don‚Äôt need to fine-tune the model on your data to use it. It can directly return probability scores for any list of labels you want.)\n",
    "\n",
    "In the above example, we classified using 2 labels: positive and negative. But now I'll show you how to classify text using ANY set of labels of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': \"History isn't really a fun class\",\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.7267491817474365, 0.19325977563858032, 0.07999099045991898]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier(\n",
    "    \"History isn't really a fun class\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also choose a specific model for a specific task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"So today, I just feel like I need to make sure that I am getting all of my 2016 goals accomplished. \\xa0I have my goals for 2016, but I didn't get everything accomplished. \\xa0So, I want to make sure that I can see what I have accomplished.\\n\\nI need to get more into the habit of writing in my journal, I think that will help me get through a lot of things. \\xa0I know that when I feel like I am not feeling good, I can just go into my journal and take a look at what I should be doing. \\xa0It will help me see where I need to improve.\\n\\nI am going to post the 2016 goals that I have listed in this post. \\xa0I have listed the goals for each day, and will have a few goals for each week and month.\\n\\nI want to be able to see these goals, so I can see where I am and where I need to improve. \\xa0I know that if I don't see these, I will feel discouraged and will not be able to get through a lot of things.\\n\\nI want to be able to see how far I have come, so even if I didn't accomplish\"},\n",
       " {'generated_text': \"So today, I just feel like I'm being told what to do by my own government. I'm tired of the government telling me what I can do in my own home.\\n\\nThis is a scary thing.\\n\\nAnd I'm just so tired of all the BS that's going on right now.\\n\\nI thought we were living in a world where we could be free of government now. After all, we've been over this before. We've had our rights and freedoms. We've had our rights and freedoms taken away. And now, we're being told what to do by the government.\\n\\nI know this isn't a new concept. I've been telling my kids this for years. It's been on the news. It's on the radio. It's being taught in our schools. And I'm tired of it.\\n\\nI want to be free of government. I want to be free of this government tyranny. I want to be free of the government that tells me what to do.\\n\\nI want to be free of this government that is telling me what to do.\\n\\nI want to be free of this government that tells me what to do.\\n\\nI want to be free of this government that tells me what to do.\\n\\nI\"}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"HuggingFaceTB/SmolLM2-360M\") # all these models names\n",
    "# are names directly from the hugging face hub's website. You can plug in ANY model from their hub.\n",
    "# It includes all possible basic models too, that you could ever think of.\n",
    "\n",
    "# This is text generation, where you just provide some prompt, and the model auto-completes it by generating the remaining text.\n",
    "# Similar to text predictions you see on your phone.\n",
    "# Text generation is random, and it is unlikely you will get the exact same responses every time.\n",
    "\n",
    "generator(\n",
    "    \"So today, I just feel like\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are SO many other tasks and examples I could show you, that I will not be exhaustively going over them here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining data from multiple sources\n",
    "One powerful application of Transformer models is their ability to combine and process data from multiple sources. This is especially useful when you need to:\n",
    "\n",
    "- Search across multiple databases or repositories\n",
    "- Consolidate information from different formats (text, images, audio)\n",
    "- Create a unified view of related information\n",
    "\n",
    "For example, you could build a system that:\n",
    "- Searches for information across databases in multiple modalities like text and image.\n",
    "- Combines results from different sources into a single coherent response. For example, from an audio file and text description.\n",
    "- Presents the most relevant information from a database of documents and metadata.\n",
    "\n",
    "This is an import aspect of transformers, and is just something worth noting.\n",
    "Next, we're going to get into the really good stuff, the nitty-gritty, how transformers ACTUALLY work, whats goes on in the inside, and transformer model architecture, in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Architecture\n",
    "- Transformer architecture is mainly composed of 2 blocks/layers/models:\n",
    "  - Encoder layer (left): The encoder receives an input and builds a representation of it (its features). This means that the model is optimized to acquire understanding from the input.\n",
    "  - Decoder (right): The decoder uses the encoder‚Äôs representation (features) along with other inputs to generate a target sequence. This means that the model is optimized for generating outputs. \n",
    "- Here's an image for visualisation\n",
    "\n",
    "![Transformer Architecture Visualisation](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks-dark.svg)\n",
    "- Each of these parts can be used independently too, depending on the task:\n",
    "  - Encoder-only models: Good for tasks that require understanding of the input, such as sentence classification and named entity recognition.\n",
    "  - Decoder-only models: Good for generative tasks such as text generation.\n",
    "  - Encoder-decoder models or sequence-to-sequence models: Good for generative tasks that require an input, such as translation or summarization (what you see with pure ChatGPT).\n",
    "- Another key aspect of Transformer models are **Attention layers**.\n",
    "  - By the way, the transformer architecture was introduced in a Google paper titled \"Attention is All You Need\". Says a lot.\n",
    "-  This layer  will tell the model to pay specific attention to certain parts of the data (e.g. with LLMs, that would be words in the sentence) that you passed it (and more or less ignore the others) when dealing with the representation of each piece of data (word). \n",
    "  - Lets put this into context. Something that makes sense; imagine you have to translate text from English to French. The input is \"You like this house\", the translation model will need to look at \"you\" to get the proper translation for the word \"like\", because in French, the word \"like\" is written differently depending on the subject. But, the rest of the sentence is not useful for the translation of \"like\" in this case.\n",
    "  - In the same way, when translating ‚Äúthis‚Äù the model will also need to pay attention to the word \"house\", because ‚Äúthis‚Äù translates differently depending on whether the associated noun is masculine or feminine. Again, the other words in the sentence will not matter for the translation of ‚Äúhouse‚Äù. \n",
    "  - With more complex sentences (and more complex grammar rules), the model would need to pay special attention to words that might appear farther away in the sentence to properly translate each word.\n",
    "  - This same concept applies to ANY task at ANY scale associated with natural language: a word by itself has a meaning, but that meaning is deeply affected by the context, which can be any other word (or words) before or after the word being studied.\n",
    "  - Here's a code visualization and application of our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The `output_attentions` attribute is not supported when using the `attn_implementation` set to sdpa. Please set it to 'eager' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m MarianMTModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mattn_implementation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meager\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# üëà forces attention outputs to work\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_attentions\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Input sentence\u001b[39;00m\n\u001b[1;32m     13\u001b[0m src_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou like this house\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages/transformers/configuration_utils.py:204\u001b[0m, in \u001b[0;36mPretrainedConfig.__setattr__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    203\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m)[key]\n\u001b[0;32m--> 204\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__setattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages/transformers/configuration_utils.py:341\u001b[0m, in \u001b[0;36mPretrainedConfig.output_attentions\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;129m@output_attentions\u001b[39m\u001b[38;5;241m.\u001b[39msetter\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moutput_attentions\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meager\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 341\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    342\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `output_attentions` attribute is not supported when using the `attn_implementation` set to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    343\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attn_implementation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please set it to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meager\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    344\u001b[0m         )\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_attentions \u001b[38;5;241m=\u001b[39m value\n",
      "\u001b[0;31mValueError\u001b[0m: The `output_attentions` attribute is not supported when using the `attn_implementation` set to sdpa. Please set it to 'eager' instead."
     ]
    }
   ],
   "source": [
    "from transformers import MarianTokenizer, MarianMTModel\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "model.config.attn_implementation = \"eager\"  # üëà forces attention outputs to work\n",
    "model.config.output_attentions = True\n",
    "# Input sentence\n",
    "src_text = \"You like this house\"\n",
    "tgt_text = \"Vous aimez cette maison\"  # Teacher forcing target\n",
    "\n",
    "# Tokenize input and target\n",
    "inputs = tokenizer(src_text, return_tensors=\"pt\")\n",
    "with tokenizer.as_target_tokenizer():\n",
    "    labels = tokenizer(tgt_text, return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass with teacher forcing to get cross-attention\n",
    "outputs = model(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    labels=labels[\"input_ids\"],\n",
    "    output_attentions=True,\n",
    "    return_dict=True\n",
    ")\n",
    "\n",
    "# Extract cross-attention from first layer, first head\n",
    "cross_attn = outputs.cross_attentions[0][0, 0]  # shape: [target_len, source_len]\n",
    "\n",
    "# Decode tokens\n",
    "input_tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "target_tokens = tokenizer.convert_ids_to_tokens(labels[\"input_ids\"][0])\n",
    "\n",
    "# Plot cross-attention\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(cross_attn.detach().numpy(), xticklabels=input_tokens, yticklabels=target_tokens, cmap=\"viridis\")\n",
    "plt.xlabel(\"Input (English)\")\n",
    "plt.ylabel(\"Target (French)\")\n",
    "plt.title(\"Cross-Attention Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the above image, we see a self-attention matrix from BERT (Layer 1, Head 1). Here‚Äôs how to read it:\n",
    "\t-\tY-axis (Query): The token whose attention is being calculated (e.g. ‚Äúlike‚Äù, ‚Äúhouse‚Äù).\n",
    "\t-\tX-axis (Key): The tokens that receive attention from the query.\n",
    "\t-\tColor intensity: Strength of attention weight (brighter = more attention).\n",
    "\n",
    "In short, a row shows which tokens a particular word is focusing on.\n",
    "- To further accelerate our understanding, we will briefly look at the original transformer architecture:\n",
    "  - Original transformer architecture was designed for translation.\n",
    "  - During training, the encoder receives inputs (sentences) in a certain language, while the decoder receives the same sentences in the desired target language. \n",
    "  - In the encoder, the attention layers can use all the words in a sentence (since, as we just saw, the translation of a given word can be dependent on what is after as well as before it in the sentence). \n",
    "  - The decoder, however, works sequentially and can only pay attention to the words in the sentence that it has already translated (so, only the words before the word currently being generated). \n",
    "    - For example, when we have predicted the first three words of the translated target, we give them to the decoder which then uses all the inputs of the encoder to try to predict the fourth word.\n",
    "  - To speed things up during training (when the model has access to target sentences), the decoder is fed the whole target, but it is not allowed to use future words (if it had access to the word at position 2 when trying to predict the word at position 2, the problem would not be very hard!).\n",
    "  - For instance, when trying to predict the fourth word, the attention layer will only have access to the words in positions 1 to 3.\n",
    "  - Here's a visualisation of the original Transformer architecture, with the encoder on the left and the decoder on the right:\n",
    "![Original Transformer Architecture Visualisation](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers-dark.svg)\n",
    "  - Note that the first attention layer in a decoder block pays attention to all (past) inputs to the decoder, but the second attention layer uses the output of the encoder. It can thus access the whole input sentence to best predict the current word.\n",
    "    - This is very useful as different languages can have grammatical rules that put the words in different orders, or some context provided later in the sentence may be helpful to determine the best translation of a given word.\n",
    "  - The attention mask can also be used in the encoder/decoder to prevent the model from paying attention to some special words ‚Äî for instance, the special padding word used to make all the inputs the same length when batching together sentences.\n",
    "  - Hopefully, this gave you a solid mental model of how the original Transformer architecture works ‚Äî particularly the encoder-decoder setup and how attention mechanisms shape language understanding and generation. We covered this not just for historical context, but because these components (like self-attention, masking, and encoder-decoder flows) form the foundation of nearly all modern Transformer-based models. \n",
    "    - Understanding them now will make it much easier to grasp variants like GPT, BERT, or T5, and to debug, fine-tune, or even build your own architectures later on.\n",
    "  #### Architectures and checkpoints\n",
    "  - This is terminology that is important to know for now and later. You‚Äôll see mentions of architectures and checkpoints as well as models. These terms all have slightly different meanings:\n",
    "    - **Architecture**: This is the skeleton of the model ‚Äî the definition of each layer and each operation that happens within the model.\n",
    "    - **Checkpoints**: These are the weights that will be loaded in a given architecture.\n",
    "    - **Model**: This is an umbrella term that isn‚Äôt as precise as ‚Äúarchitecture‚Äù or ‚Äúcheckpoint\": it can mean both. This course will specify architecture or checkpoint when it matters to reduce ambiguity.\n",
    "    - Example: BERT is an architecture while bert-base-cased, a set of weights trained by the Google team for the first release of BERT, is a checkpoint. However, one can say ‚Äúthe BERT model‚Äù and ‚Äúthe bert-base-cased model.‚Äù\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How transformers solve their tasks\n",
    "- Now, we will delve into specific transformer architectural variants, and how they solve their respective, specific tasks. Before we do this however, its important to understand that most tasks follow a similar pattern: input data is processed through a model, and the output is interpreted for a specific task. The differences lie in how the data is prepared, what model architecture variant is used, and how the output is processed.\n",
    "- To explain how tasks are solved, we‚Äôll walk through what goes on inside the model to output useful predictions. We‚Äôll cover the following models and their corresponding tasks:\n",
    "\n",
    "  - Wav2Vec2 for audio classification and automatic speech recognition (ASR)\n",
    "  - Vision Transformer (ViT) and ConvNeXT for image classification\n",
    "  - DETR for object detection\n",
    "  - Mask2Former for image segmentation\n",
    "  - GLPN for depth estimation\n",
    "  - BERT for NLP tasks like text classification, token classification and question answering that use an encoder\n",
    "  - GPT2 for NLP tasks like text generation that use a decoder\n",
    "  - BART for NLP tasks like summarization and translation that use an encoder-decoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Language models and the most famous examples:\n",
    "- All the popular Transformer models (GPT, BERT, T5, etc.) have been trained as language models. \n",
    "- Language models are just models that have been trained on large amounts of raw text in a self-supervised fashion.\n",
    "- (Self-supervised learning is a type of training in which the objective is automatically computed from the inputs of the model. That means that humans are not needed to label the data!)\n",
    "- This type of model develops a statistical understanding of the language it has been trained on, but it‚Äôs less useful for specific practical tasks. Because of this, the general pretrained model then goes through a process called transfer learning or fine-tuning. During this process, the model is fine-tuned in a supervised way ‚Äî that is, using human-annotated labels ‚Äî on a given task.\n",
    "- An example of a task is predicting the next word in a sentence having read the n previous words. This is called causal language modeling because the output depends on the past and present inputs, but not the future ones.\n",
    "- Example below:\n",
    "\n",
    "![Causal language modeling visualization](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling-dark.svg)\n",
    "\n",
    "- Additionally, the general strategy to achieve better performance is by increasing the models‚Äô sizes as well as the amount of data they are pretrained on.\n",
    "- Unfortunately, training a model, especially a large one, requires a large amount of data. This becomes very costly in terms of time and compute resources.\n",
    "- This means sharing models/resources is very efficient and optimal, because it saves time, potentially money, and just overall resources for everyone.\n",
    "\n",
    "## Transfer Learning\n",
    "- Initializing a model with another model's weights.\n",
    "- Essentially, you leverage the knowledge acquired by a model trained on LOTS of data on another task.\n",
    "### Pre-training\n",
    "- **Pretraining**: training a model from scratch. The weights are initially randomly initialized, and the training starts with 0 knowledge.\n",
    "  - This pretraining is usually done on very large amounts of data. Therefore, it requires a very large corpus of data, and training can take up to several weeks.\n",
    "- **Fine-tuning**: training done AFTER a model has been pretrained. To perform fine-tuning, you first need a pretrained language model, then need to perform additional training with a dataset highly specific to your task.\n",
    "  - Now, this might seem confusing. Just train your model for your final use case from the start right? Here's some reasons why you do things this way:\n",
    "  - The pretrained model was already trained on a dataset that has some similarities with the fine-tuning dataset. The fine-tuning process is thus able to take advantage of knowledge acquired by the initial model during pretraining (for instance, with NLP problems, the pretrained model will have some kind of statistical understanding of the language you are using for your task).\n",
    "  - Since the pretrained model was already trained on lots of data, the fine-tuning requires way less data to get decent results.\n",
    "  - For the same reason, the amount of time and resources needed to get good results are much lower.\n",
    "  - Fine-tuning example:\n",
    "    - You can take a pretrained model (that was thoroughly trained on the English language), and fine-tune it on the arXiv corpus, which then results in a science/research-based model.\n",
    "      - Corpus: a large and structured collection of text or speech data used for linguistic analysis and training ML models. In NLP/LLM context, it acts as the training data for our models that need to generate human language. \n",
    "      - arXiv corpus: arXiv is an archive of scholarly articles, primarily in scientific fields. So the arXiv corpus is a collection of scholarly articles available on the arXiv repository. \n",
    "    - The fine-tuning will only require a limited amount of data. The knowledge the pretrained model has acquired is ‚Äútransferred,‚Äù (You‚Äôre transferring the capabilities of the pretrained model to a new domain/task using the arXiv data), hence the term transfer learning.\n",
    "- Important to note, that transfer learning **ENCOMPASSES** fine-tuning.\n",
    "- In most situations, fine-tuning revolves around retraining the last couple layers of a model, but can be applied to the whole model.  \n",
    "- Fine-tuning a model has lower time, data, financial, and environmental costs. It is also quicker and easier to iterate over different fine-tuning schemes (full fine-tuning, last-layer tuning, freezing first n-layers and training/readjusting the rest, peft, etc), as the training is less constraining (cheaper, faster, easier to experiment with) than a full pretraining.\n",
    "- This process will also achieve better results than training from scratch (unless you have lots of data), which is why you should always try to leverage a pretrained model ‚Äî one as close as possible to the task you have at hand ‚Äî and fine-tune it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
