{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfomers\n",
    "- Transformers are perhaps one of the most special, impactful and important creations in AI/Computer Science of **all time**.\n",
    "- Transformer models are used to solve all kinds of tasks across different modalities, including natural language processing (NLP), computer vision, audio processing, and more.\n",
    "\n",
    "### Hugging Face Transformers Library\n",
    "\n",
    "The **Hugging Face Transformers** library is the most important and widely-used library for working with transformer models in the AI/ML ecosystem.\n",
    "\n",
    "### What is it?\n",
    "- A comprehensive Python library that provides easy access to thousands of pre-trained transformer models\n",
    "- Supports all major transformer architectures: BERT, GPT, T5, RoBERTa, DistilBERT, and many more\n",
    "- Provides a unified, consistent API regardless of the underlying model architecture\n",
    "- Includes tools for fine-tuning, training from scratch, and deploying models to production\n",
    "\n",
    "### Why it's significant\n",
    "The library democratized AI by making state-of-the-art transformer models accessible to developers, researchers, and companies worldwide. It created a standardized interface that works across different model families and tasks, building the largest model hub with 100,000+ pre-trained models shared by the community. This massive ecosystem is now used by thousands of companies in real-world applications.\n",
    "\n",
    "### The Pipeline Function - The Game Changer\n",
    "\n",
    "- The `pipeline()` function is the most simple object in the transformers library, yet perhaps the most revolutionary feature - it's like having a \"one-click\" solution for AI tasks. It handles tokenization, model loading, and post-processing automatically, supporting 20+ different tasks (classification, generation, translation, summarization, etc.) with zero configuration needed.\n",
    "- You can think of it as something that connects a model with its necessary preprocessing and postprocessing steps. This lets us directly input any text and get an intelligible answer.\n",
    "- Below I will show you an example of sentiment analysis using the pipeline function, on our own inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting einops (from -r transformers_requirements.txt (line 1))\n",
      "  Using cached einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: torch in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from -r transformers_requirements.txt (line 3)) (2.7.1)\n",
      "Requirement already satisfied: transformers[sentencepiece] in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from -r transformers_requirements.txt (line 2)) (4.53.0)\n",
      "Requirement already satisfied: filelock in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: protobuf in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (6.31.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (1.1.5)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from torch->-r transformers_requirements.txt (line 3)) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from torch->-r transformers_requirements.txt (line 3)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from torch->-r transformers_requirements.txt (line 3)) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from sympy>=1.13.3->torch->-r transformers_requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from jinja2->torch->-r transformers_requirements.txt (line 3)) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from requests->transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from requests->transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from requests->transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages (from requests->transformers[sentencepiece]->-r transformers_requirements.txt (line 2)) (2025.6.15)\n",
      "Using cached einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Installing collected packages: einops\n",
      "Successfully installed einops-0.8.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r transformers_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atin5551/Documents/GitHub/Applied AI ML Implementations/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9931273460388184}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\") # there are SO many differents tasks you could name here\n",
    "# by default, a particular pretrained model that has been fine-tuned for sentiment analysis in English gets chosen\n",
    "# here, sentinment-analysis defaults to distilbert-base-uncased-finetuned-sst-2-english,\n",
    "# which is part of the BERT model lines (by Google)\n",
    "classifier(\"I'm so hungry man!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9961236119270325},\n",
       " {'label': 'POSITIVE', 'score': 0.9998375177383423}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can even do multiple outputs\n",
    "classifier(\n",
    "    [\"I'm so hungry man.\", \"I love food so much.\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth noting that the model gets downloaded and cached when you create the classifier object. Now, everytime you rerun the command, the cached model gets used instead, no need for repeated downloads.\n",
    "Here's what happens everytime you pass some text into a pipeline:\n",
    "- The text is preprocessed into a format the model can understand.\n",
    "- The preprocessed inputs are passed to the model.\n",
    "- The predictions of the model are post-processed, so you can make sense of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also worth noting the different tasks you can do with the pipeline object. Below are some examples:\n",
    "\n",
    "Text pipelines\n",
    "- text-generation: Generate text from a prompt\n",
    "- text-classification: Classify text into predefined categories\n",
    "- summarization: Create a shorter version of a text while preserving key information\n",
    "- translation: Translate text from one language to another\n",
    "- zero-shot-classification: Classify text without prior training on specific labels\n",
    "- feature-extraction: Extract vector representations of text\n",
    "\n",
    "Image pipelines\n",
    "- image-to-text: Generate text descriptions of images\n",
    "- image-classification: Identify objects in an image\n",
    "- object-detection: Locate and identify objects in images\n",
    "\n",
    "Audio pipelines\n",
    "- automatic-speech-recognition: Convert speech to text\n",
    "- audio-classification: Classify audio into categories\n",
    "- text-to-speech: Convert text to spoken audio\n",
    "\n",
    "Multimodal pipelines\n",
    "- image-text-to-text: Respond to an image based on a text prompt\n",
    "\n",
    "Now, lets try classifying texts that haven't been labelled. We're going to use something called zero-shot classification, as it allows us to specify which labels to use for classification, so we don't have to rely on pretrained models' labels. \n",
    "- (The pipeline is called zero-shot because you don’t need to fine-tune the model on your data to use it. It can directly return probability scores for any list of labels you want.)\n",
    "\n",
    "In the above example, we classified using 2 labels: positive and negative. But now I'll show you how to classify text using ANY set of labels of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': \"History isn't really a fun class\",\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.7267491817474365, 0.19325977563858032, 0.07999099045991898]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier(\n",
    "    \"History isn't really a fun class\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also choose a specific model for a specific task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"So today, I just feel like I need to make sure that I am getting all of my 2016 goals accomplished. \\xa0I have my goals for 2016, but I didn't get everything accomplished. \\xa0So, I want to make sure that I can see what I have accomplished.\\n\\nI need to get more into the habit of writing in my journal, I think that will help me get through a lot of things. \\xa0I know that when I feel like I am not feeling good, I can just go into my journal and take a look at what I should be doing. \\xa0It will help me see where I need to improve.\\n\\nI am going to post the 2016 goals that I have listed in this post. \\xa0I have listed the goals for each day, and will have a few goals for each week and month.\\n\\nI want to be able to see these goals, so I can see where I am and where I need to improve. \\xa0I know that if I don't see these, I will feel discouraged and will not be able to get through a lot of things.\\n\\nI want to be able to see how far I have come, so even if I didn't accomplish\"},\n",
       " {'generated_text': \"So today, I just feel like I'm being told what to do by my own government. I'm tired of the government telling me what I can do in my own home.\\n\\nThis is a scary thing.\\n\\nAnd I'm just so tired of all the BS that's going on right now.\\n\\nI thought we were living in a world where we could be free of government now. After all, we've been over this before. We've had our rights and freedoms. We've had our rights and freedoms taken away. And now, we're being told what to do by the government.\\n\\nI know this isn't a new concept. I've been telling my kids this for years. It's been on the news. It's on the radio. It's being taught in our schools. And I'm tired of it.\\n\\nI want to be free of government. I want to be free of this government tyranny. I want to be free of the government that tells me what to do.\\n\\nI want to be free of this government that is telling me what to do.\\n\\nI want to be free of this government that tells me what to do.\\n\\nI want to be free of this government that tells me what to do.\\n\\nI\"}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"HuggingFaceTB/SmolLM2-360M\") # all these models names\n",
    "# are names directly from the hugging face hub's website. You can plug in ANY model from their hub.\n",
    "# It includes all possible basic models too, that you could ever think of.\n",
    "\n",
    "# This is text generation, where you just provide some prompt, and the model auto-completes it by generating the remaining text.\n",
    "# Similar to text predictions you see on your phone.\n",
    "# Text generation is random, and it is unlikely you will get the exact same responses every time.\n",
    "\n",
    "generator(\n",
    "    \"So today, I just feel like\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are SO many other tasks and examples I could show you, that I will not be exhaustively going over them here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining data from multiple sources\n",
    "One powerful application of Transformer models is their ability to combine and process data from multiple sources. This is especially useful when you need to:\n",
    "\n",
    "- Search across multiple databases or repositories\n",
    "- Consolidate information from different formats (text, images, audio)\n",
    "- Create a unified view of related information\n",
    "\n",
    "For example, you could build a system that:\n",
    "- Searches for information across databases in multiple modalities like text and image.\n",
    "- Combines results from different sources into a single coherent response. For example, from an audio file and text description.\n",
    "- Presents the most relevant information from a database of documents and metadata.\n",
    "\n",
    "This is an import aspect of transformers, and is just something worth noting.\n",
    "Next, we're going to get into the really good stuff, the nitty-gritty, how transformers ACTUALLY work, whats goes on in the inside, and transformer model architecture, in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Architecture\n",
    "\n",
    "\n",
    "## Language models and the most famous examples:\n",
    "- All the popular Transformer models (GPT, BERT, T5, etc.) have been trained as language models. \n",
    "- Language models are just models that have been trained on large amounts of raw text in a self-supervised fashion.\n",
    "- (Self-supervised learning is a type of training in which the objective is automatically computed from the inputs of the model. That means that humans are not needed to label the data!)\n",
    "- This type of model develops a statistical understanding of the language it has been trained on, but it’s less useful for specific practical tasks. Because of this, the general pretrained model then goes through a process called transfer learning or fine-tuning. During this process, the model is fine-tuned in a supervised way — that is, using human-annotated labels — on a given task.\n",
    "- An example of a task is predicting the next word in a sentence having read the n previous words. This is called causal language modeling because the output depends on the past and present inputs, but not the future ones.\n",
    "- Example below:\n",
    "\n",
    "![Causal language modeling visualization](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling-dark.svg)\n",
    "\n",
    "- Additionally, the general strategy to achieve better performance is by increasing the models’ sizes as well as the amount of data they are pretrained on.\n",
    "- Unfortunately, training a model, especially a large one, requires a large amount of data. This becomes very costly in terms of time and compute resources.\n",
    "- This means sharing models/resources is very efficient and optimal, because it saves time, potentially money, and just overall resources for everyone.\n",
    "\n",
    "## Transfer Learning\n",
    "- \n",
    "### Pre-training\n",
    "- **Pretraining**: training a model from scratch. The weights are initially randomly initialized, and the training starts with 0 knowledge.\n",
    "  - This pretraining is usually done on very large amounts of data. Therefore, it requires a very large corpus of data, and training can take up to several weeks.\n",
    "- **Fine-tuning**: training done AFTER a model has been pretrained. To perform fine-tuning, you first need a pretrained language model, then need to perform additional training with a dataset highly specific to your task.\n",
    "  - Now, this might seem confusing. Just train your model for your final use case from the start right? Here's some reasons why you do things this way:\n",
    "  - The pretrained model was already trained on a dataset that has some similarities with the fine-tuning dataset. The fine-tuning process is thus able to take advantage of knowledge acquired by the initial model during pretraining (for instance, with NLP problems, the pretrained model will have some kind of statistical understanding of the language you are using for your task).\n",
    "  - Since the pretrained model was already trained on lots of data, the fine-tuning requires way less data to get decent results.\n",
    "  - For the same reason, the amount of time and resources needed to get good results are much lower."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
