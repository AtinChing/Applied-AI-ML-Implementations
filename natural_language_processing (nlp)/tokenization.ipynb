{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization & Text Preprocessing\n",
    "\n",
    "Tokenization is the foundational first step in turning raw text into a format that machine learning models ‚Äî especially LLMs ‚Äî can understand. Before anything can be embedded, classified, or generated, it has to be *tokenized*.\n",
    "\n",
    "This notebook covers:\n",
    "- What tokens are and why they matter\n",
    "- How different tokenizers (WordPiece, BPE, SentencePiece) work\n",
    "- How tokenization affects model input length, truncation, and padding\n",
    "- Real examples using Hugging Face tokenizers\n",
    "\n",
    "üìç This is part of the `fundamentals/` series to build a solid foundation for working with NLP, LLMs, and vector-based search pipelines.\n",
    "\n",
    "Before we proceed with this, it is very important to first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Plans:\n",
    "-\tHow tokenization is implemented\n",
    "-\tHow raw text becomes token sequences\n",
    "-\tWhy subwords exist\n",
    "-\tBPE merges, vocab lookup etc.\n",
    "- BPE/WordPiece, vocab files, token IDs, types of tokenization"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
